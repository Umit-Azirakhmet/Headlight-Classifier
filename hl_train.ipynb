{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# print('System Version:', sys.version)\n",
    "# print('PyTorch version', torch.__version__)\n",
    "# print('Torchvision version', torchvision.__version__)\n",
    "# print('Numpy version', np.__version__)\n",
    "# print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 20,\n",
    "    \"image_size\": (128, 128),\n",
    "    \"patience\": 8, \n",
    "    \"factor\": 0.4\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=128, width=128),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=10, p=1.0),\n",
    "        A.Perspective(scale=0.0001, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=128, width=128),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlightDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data.imgs[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HeadlightDataset(\n",
    "    data_dir='./dataset/train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, label = dataset[600]\n",
    "# print(label)\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'off', 1: 'on'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = './dataset/train'\n",
    "target_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\n",
    "print(target_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = './dataset/train'\n",
    "dataset = HeadlightDataset(data_dir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mobilenet\n",
    "class HeadlightClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(HeadlightClassifier, self).__init__()\n",
    "        self.base_model = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.base_model.features[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        in_features = self.base_model.classifier[0].in_features\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_features, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.additional_layers(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #efficientnet\n",
    "# import timm\n",
    "\n",
    "# class HeadlightClassifier(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(HeadlightClassifier, self).__init__()\n",
    "#         self.base_model = timm.create_model('efficientnetv2_rw_s', pretrained=True, num_classes=0)\n",
    "\n",
    "#         for param in self.base_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         for param in self.base_model.blocks[-1].parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         in_features = self.base_model.num_features\n",
    "#         self.additional_layers = nn.Sequential(\n",
    "#             nn.Conv2d(in_features, 512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         )\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.base_model.forward_features(x)\n",
    "#         x = self.additional_layers(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #resnet\n",
    "# class HeadlightClassifier(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(HeadlightClassifier, self).__init__()\n",
    "#         self.base_model = models.resnet18(pretrained=True)\n",
    "\n",
    "#         for param in self.base_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         for param in self.base_model.layer4.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         in_features = self.base_model.fc.in_features\n",
    "#         self.base_model.fc = nn.Identity()\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(in_features, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.base_model(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_folder = './dataset/train'\n",
    "valid_folder = './dataset/val'\n",
    "#test_folder = '../input/cards-image-datasetclassification/test/'\n",
    "\n",
    "train_transform = get_train_transform()\n",
    "valid_transform = get_valid_transform()\n",
    "\n",
    "train_dataset = HeadlightDataset(train_folder, transform=transform)\n",
    "val_dataset = HeadlightDataset(valid_folder, transform=transform)\n",
    "#test_dataset = HeadlightDataset(test_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "#test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HeadlightClassifier(num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image, transform(image).unsqueeze(0)\n",
    "\n",
    "def predict(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        # print(torch.max(image_tensor))\n",
    "        # print(torch.min(image_tensor))\n",
    "        # print(image_tensor.shape)\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    return probabilities.cpu().numpy().flatten()\n",
    "\n",
    "def visualize_predictions(original_image, probabilities, class_names):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    axarr[0].imshow(original_image)\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "    axarr[1].barh(class_names, probabilities)\n",
    "    axarr[1].set_xlabel(\"Probability\")\n",
    "    axarr[1].set_title(\"Class Predictions\")\n",
    "    axarr[1].set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_image = \"./dataset/test/on/frame_890_car_357.jpg\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "original_image, image_tensor = preprocess_image(test_image, transform)\n",
    "probabilities = predict(model, image_tensor, device)\n",
    "\n",
    "class_names = dataset.classes \n",
    "\n",
    "print(class_names)\n",
    "print(probabilities)\n",
    "\n",
    "visualize_predictions(original_image, probabilities, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "test_images = glob('./dataset/test/*/*')\n",
    "test_examples = np.random.choice(test_images, 20)\n",
    "\n",
    "for example in test_examples:\n",
    "    original_image, image_tensor = preprocess_image(example, transform)\n",
    "    probabilities = predict(model, image_tensor, device)\n",
    "    class_names = dataset.classes\n",
    "    #print(probabilities)\n",
    "    visualize_predictions(original_image, probabilities, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving img, graphs\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from glob import glob\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "\n",
    "# output_dir = './predictions/resnet_graph'  # change name of output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# test_images = glob('./dataset/test/*/*')\n",
    "\n",
    "# for example in test_images:\n",
    "#     original_image, image_tensor = preprocess_image(example, transform)\n",
    "#     probabilities = predict(model, image_tensor, device)\n",
    "#     print(probabilities)\n",
    "#     image_filename = os.path.basename(example)\n",
    "#     save_path = os.path.join(output_dir, f\"{image_filename.split('.')[0]}_prediction.png\")\n",
    "\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "#     # Display the image\n",
    "#     ax1.imshow(original_image)\n",
    "#     ax1.axis('off')\n",
    "    \n",
    "#     # Display the probabilities bar chart\n",
    "#     ax2.barh(class_names, probabilities)\n",
    "#     ax2.set_xlim(0, 1)\n",
    "#     ax2.set_xlabel('Probability')\n",
    "#     ax2.set_title('Class Predictions')\n",
    "\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checkpoints\n",
    "# import os\n",
    "\n",
    "# checkpoint_dir = 'checkpoints'\n",
    "# checkpoint_file = os.path.join(checkpoint_dir, 'mobilenet_checkpoint.pth')  #change name of checkpoint\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# torch.save({\n",
    "#     'epoch': epoch + 1,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'train_loss': train_losses,\n",
    "#     'val_loss': val_losses\n",
    "# }, checkpoint_file)\n",
    "\n",
    "# print(f'Model checkpoint saved to {checkpoint_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 81.93%\n",
      "Test Accuracy: 76.74%\n",
      "Results saved to ./accuracy_results_check.md\n"
     ]
    }
   ],
   "source": [
    "#Checking Accuracy\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "validation_dataset = datasets.ImageFolder(root='./dataset/val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='./dataset/test', transform=transform)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "model_name = model.__class__.__name__ #not working, remove\n",
    "\n",
    "validation_accuracy = evaluate_model(model, validation_loader, device)\n",
    "test_accuracy = evaluate_model(model, test_loader, device)\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "results = f\"## Model: {model_name}\\n\"\n",
    "results += f\"**Date:** {current_time}\\n\\n\"\n",
    "results += f\"**Validation Accuracy:** {validation_accuracy * 100:.2f}%\\n\\n\"\n",
    "results += f\"**Test Accuracy:** {test_accuracy * 100:.2f}%\\n\\n\\n\\n\"\n",
    "output_path = './accuracy_results_check.md'\n",
    "\n",
    "\n",
    "with open(output_path, 'a') as f:\n",
    "    f.write(results)\n",
    "\n",
    "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
